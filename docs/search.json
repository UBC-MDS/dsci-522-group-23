[
  {
    "objectID": "notebooks/report.html",
    "href": "notebooks/report.html",
    "title": "Predicting academic performance using demographic and behavioral Data",
    "section": "",
    "text": "This project investigates whether a student’s mathematics performance can be predicted using demographic and behavioral data, aiming to help educators in supporting students and tailoring educational strategies. Using a Ridge Regression model with optimized hyperparameters (alpha = 1), we achieved a strong predictive accuracy with a cross-validation score of 0.81 and evaluation metrics on the test set including an MSE of 4.048, RMSE of 2.012, and MAE of 1.309. While the model demonstrates robust performance, future work could explore non-linear models and provide confidence intervals for predictions to enhance interpretability and reliability, ultimately contributing to better educational outcomes."
  },
  {
    "objectID": "notebooks/report.html#summary",
    "href": "notebooks/report.html#summary",
    "title": "Predicting academic performance using demographic and behavioral Data",
    "section": "",
    "text": "This project investigates whether a student’s mathematics performance can be predicted using demographic and behavioral data, aiming to help educators in supporting students and tailoring educational strategies. Using a Ridge Regression model with optimized hyperparameters (alpha = 1), we achieved a strong predictive accuracy with a cross-validation score of 0.81 and evaluation metrics on the test set including an MSE of 4.048, RMSE of 2.012, and MAE of 1.309. While the model demonstrates robust performance, future work could explore non-linear models and provide confidence intervals for predictions to enhance interpretability and reliability, ultimately contributing to better educational outcomes."
  },
  {
    "objectID": "notebooks/report.html#introduction",
    "href": "notebooks/report.html#introduction",
    "title": "Predicting academic performance using demographic and behavioral Data",
    "section": "2 Introduction",
    "text": "2 Introduction\nMath teaches us to think logically and it also provides us with analytical and problem-solving skills. These skills can be applied to various academic and professional fields. However, student performance in mathematics can be influenced by many factors, like individual factor, social factor, and family factor. Research has shown that attributes such as study habits, age, social behaviour (alcohol consumptions, etc) and family background can significantly impact a student’s academic success. Understanding these factors is crucial for improving educational outcomes. (Bitrus, Apagu, and Hamsatu (2016), Hjarnaa et al. (2023), Modi (2023))\nIn this study, we aim to address this question: “Can we predict a student’s math academic performance based on the demographic and behavioral data?”. Answering this question is important because understanding the factors behind student performance can help teachers provide support to struggling students. Furthermore, the ability to predict academic performance could assist schools in developing educational strategies based on different backgrounds of students. The goal of this study is to develop a machine learning model capable of predicting student’s math performance with high accuracy.\nThe dataset ((student_performance?)) used in this study contains detailed records of student demographics and behaviors, such as age, study habits, social behaviors, and family background. The target variable, mathematics performance, is measured as a continuous score reflecting students’ final grade. This dataset offers a great opportunity to explore meaningful relationships between features and academic outcomes."
  },
  {
    "objectID": "notebooks/report.html#methods-results",
    "href": "notebooks/report.html#methods-results",
    "title": "Predicting academic performance using demographic and behavioral Data",
    "section": "3 Methods & Results",
    "text": "3 Methods & Results\nThe objective here to prepare the data for our classification analysis by exploring relevant features and summarizing key insights through data wrangling and visualization.\n\n3.1 Dataset Description\nThe full data set contains the following columns:\n\nschool - student’s school (binary: ‘GP’ - Gabriel Pereira or ‘MS’ - Mousinho da Silveira)\nsex - student’s sex (binary: ‘F’ - female or ‘M’ - male)\nage - student’s age (numeric: from 15 to 22)\naddress - student’s home address type (binary: ‘U’ - urban or ‘R’ - rural)\nfamsize - family size (binary: ‘LE3’ - less or equal to 3 or ‘GT3’ - greater than 3)\nPstatus - parent’s cohabitation status (binary: ‘T’ - living together or ‘A’ - apart)\nMedu - mother’s education (numeric: 0 - none, 1 - primary education (4th grade), 2 - “ 5th to 9th grade, 3 - “ secondary education or 4 - “ higher education)\nFedu - father’s education (numeric: 0 - none, 1 - primary education (4th grade), 2 - “ 5th to 9th grade, 3 - “ secondary education or 4 - “ higher education)\nMjob - mother’s job (nominal: ‘teacher’, ‘health’ care related, civil ‘services’ (e.g. administrative or police), ‘at_home’ or ‘other’)\nFjob - father’s job (nominal: ‘teacher’, ‘health’ care related, civil ‘services’ (e.g. administrative or police), ‘at_home’ or ‘other’)\nreason - reason to choose this school (nominal: close to ‘home’, school ‘reputation’, ‘course’ preference or ‘other’)\nguardian - student’s guardian (nominal: ‘mother’, ‘father’ or ‘other’)\ntraveltime - home to school travel time (numeric: 1 - &lt;15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - &gt;1 hour)\nstudytime - weekly study time (numeric: 1 - &lt;2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - &gt;10 hours)\nfailures - number of past class failures (numeric: n if 1&lt;=n&lt;3, else 4)\nschoolsup - extra educational support (binary: yes or no)\nfamsup` - family educational support (binary: yes or no)\npaid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\nactivities - extra-curricular activities (binary: yes or no)\nnursery - attended nursery school (binary: yes or no)\nhigher - wants to take higher education (binary: yes or no)\ninternet - Internet access at home (binary: yes or no)\nromantic - with a romantic relationship (binary: yes or no)\nfamrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\nfreetime - free time after school (numeric: from 1 - very low to 5 - very high)\ngoout - going out with friends (numeric: from 1 - very low to 5 - very high)\nDalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\nWalc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\nhealth - current health status (numeric: from 1 - very bad to 5 - very good)\nabsences - number of school absences (numeric: from 0 to 93)\n\nThese columns represent the grades:\n\nG1 - first period grade (numeric: from 0 to 20)\nG2 - second period grade (numeric: from 0 to 20)\nG3 - final grade (numeric: from 0 to 20, output target)\n\nAttribution: The dataset variable description is copied as original from the UCI Machine Learning Repository.\n\n\n3.2 Data Loading, Wrangling and Summary\nLet’s start by loading the data and have an initial view of data set structure.\nThe file is a .csv file with ; as delimiter. Let’s use pandasto read it in.\n\nstudent_performance = pd.read_csv(Path('../data/raw/student-mat.csv'), sep=';')\n\nThis provides an overview of the data set with 33 columns, each representing student attributes such as age, gender, study time, grades, and parental details.\nLet’s get some information on the data set to better understand it.\n\nstudent_performance.head()\n\n\n\n\n\n\n\n\nschool\nsex\nage\naddress\nfamsize\nPstatus\nMedu\nFedu\nMjob\nFjob\n...\nfamrel\nfreetime\ngoout\nDalc\nWalc\nhealth\nabsences\nG1\nG2\nG3\n\n\n\n\n0\nGP\nF\n18\nU\nGT3\nA\n4\n4\nat_home\nteacher\n...\n4\n3\n4\n1\n1\n3\n6\n5\n6\n6\n\n\n1\nGP\nF\n17\nU\nGT3\nT\n1\n1\nat_home\nother\n...\n5\n3\n3\n1\n1\n3\n4\n5\n5\n6\n\n\n2\nGP\nF\n15\nU\nLE3\nT\n1\n1\nat_home\nother\n...\n4\n3\n2\n2\n3\n3\n10\n7\n8\n10\n\n\n3\nGP\nF\n15\nU\nGT3\nT\n4\n2\nhealth\nservices\n...\n3\n2\n2\n1\n1\n5\n2\n15\n14\n15\n\n\n4\nGP\nF\n16\nU\nGT3\nT\n3\n3\nother\nother\n...\n4\n3\n2\n1\n2\n5\n4\n6\n10\n10\n\n\n\n\n5 rows × 33 columns\n\n\n\n\nstudent_performance.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 395 entries, 0 to 394\nData columns (total 33 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   school      395 non-null    object\n 1   sex         395 non-null    object\n 2   age         395 non-null    int64 \n 3   address     395 non-null    object\n 4   famsize     395 non-null    object\n 5   Pstatus     395 non-null    object\n 6   Medu        395 non-null    int64 \n 7   Fedu        395 non-null    int64 \n 8   Mjob        395 non-null    object\n 9   Fjob        395 non-null    object\n 10  reason      395 non-null    object\n 11  guardian    395 non-null    object\n 12  traveltime  395 non-null    int64 \n 13  studytime   395 non-null    int64 \n 14  failures    395 non-null    int64 \n 15  schoolsup   395 non-null    object\n 16  famsup      395 non-null    object\n 17  paid        395 non-null    object\n 18  activities  395 non-null    object\n 19  nursery     395 non-null    object\n 20  higher      395 non-null    object\n 21  internet    395 non-null    object\n 22  romantic    395 non-null    object\n 23  famrel      395 non-null    int64 \n 24  freetime    395 non-null    int64 \n 25  goout       395 non-null    int64 \n 26  Dalc        395 non-null    int64 \n 27  Walc        395 non-null    int64 \n 28  health      395 non-null    int64 \n 29  absences    395 non-null    int64 \n 30  G1          395 non-null    int64 \n 31  G2          395 non-null    int64 \n 32  G3          395 non-null    int64 \ndtypes: int64(16), object(17)\nmemory usage: 102.0+ KB\n\n\nThe data set contains 395 observations and 33 columns covering different aspects of student demographics, academic and behavioral traits.\nWe can see that there is no missing values. There is not need to handle NAs.\nThe data set includes categorical (school, sex, Mjob) and numerical (age, G1, G2, G3) features.\nThere is a large range of features but not all of them are necessary for this analysis. Let’s proceed and select only the necessary ones.\nLet’s selected the following key columns:\n\nDemographic attributes: sex, age\nAcademic Attributes: studytime, failures, G1, G2, G3 (grades for three terms)\nBehavioral Attributes: goout (socializing), Dalc (weekday alcohol consumption), Walc (weekend alcohol consumption)\n\nWe will also split the dataset into train and test set with a 80/20 ratio. We also set random_state=123 for reproducibility.\n\n# Necessary columns\ncolumns = ['sex', \n           'age', \n           'studytime', \n           'failures', \n           'goout', \n           'Dalc', \n           'Walc',\n           'G1',\n           'G2',\n           'G3']\n\nsubset_df = student_performance[columns]\n\ntrain_df, test_df = train_test_split(\n    subset_df, test_size=0.2, random_state=123\n)\n\n\n3.2.1 Data Validation Checks\nFrom heatmap shown in Figure 1, we observe no missing values, suggesting the dataset is entirely complete.\n\n\n\n\n\n\nFigure 1: Missing Values Heatmap\n\n\n\nThe histogram in Figure 2 visualizes the spread of the target variable. This distribution is critical to understanding how the target behaves and whether any transformations are needed to ensure better model performance.\n\n\n\n\n\n\nFigure 2: Distribution of the target variable\n\n\n\n\n\n3.2.2 Checking for Outliers\nThere are few outliers in failures, Dalc, age, studytime, G2, and G1, as shown in Figure 3. These outliers are relatively few compared to the 395 entries, but could still influence model results. We will apply a StandardScaler transformation to the numeric variables, the effect of these outliers will be minimized. Therefore, we will not drop or modify these outliers at this step.\n\n\n\n\n\n\nFigure 3: Visualization of Outliers\n\n\n\n\ntrain_df.head()\n\n\n\n\n\n\n\n\nsex\nage\nstudytime\nfailures\ngoout\nDalc\nWalc\nG1\nG2\nG3\n\n\n\n\n288\nM\n18\n3\n0\n4\n1\n3\n15\n14\n14\n\n\n6\nM\n16\n2\n0\n4\n1\n1\n12\n12\n11\n\n\n226\nF\n17\n2\n0\n4\n1\n3\n16\n15\n15\n\n\n319\nF\n18\n2\n0\n4\n3\n3\n11\n11\n11\n\n\n216\nF\n17\n2\n2\n5\n2\n4\n6\n6\n4\n\n\n\n\n\n\n\n\ntrain_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 316 entries, 288 to 365\nData columns (total 10 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   sex        316 non-null    object\n 1   age        316 non-null    int64 \n 2   studytime  316 non-null    int64 \n 3   failures   316 non-null    int64 \n 4   goout      316 non-null    int64 \n 5   Dalc       316 non-null    int64 \n 6   Walc       316 non-null    int64 \n 7   G1         316 non-null    int64 \n 8   G2         316 non-null    int64 \n 9   G3         316 non-null    int64 \ndtypes: int64(9), object(1)\nmemory usage: 27.2+ KB\n\n\nLet’s get a summary of the training set we are going to use for the analysis.\n\ntrain_df.describe()\n\n\n\n\n\n\n\n\nage\nstudytime\nfailures\ngoout\nDalc\nWalc\nG1\nG2\nG3\n\n\n\n\ncount\n316.000000\n316.000000\n316.000000\n316.000000\n316.000000\n316.000000\n316.000000\n316.000000\n316.000000\n\n\nmean\n16.756329\n2.050633\n0.360759\n3.098101\n1.471519\n2.306962\n10.835443\n10.601266\n10.262658\n\n\nstd\n1.290056\n0.860398\n0.770227\n1.118330\n0.855874\n1.258904\n3.252078\n3.756797\n4.522676\n\n\nmin\n15.000000\n1.000000\n0.000000\n1.000000\n1.000000\n1.000000\n4.000000\n0.000000\n0.000000\n\n\n25%\n16.000000\n1.000000\n0.000000\n2.000000\n1.000000\n1.000000\n8.000000\n8.750000\n8.000000\n\n\n50%\n17.000000\n2.000000\n0.000000\n3.000000\n1.000000\n2.000000\n11.000000\n11.000000\n11.000000\n\n\n75%\n18.000000\n2.000000\n0.000000\n4.000000\n2.000000\n3.000000\n13.000000\n13.000000\n13.000000\n\n\nmax\n22.000000\n4.000000\n3.000000\n5.000000\n5.000000\n5.000000\n19.000000\n19.000000\n20.000000\n\n\n\n\n\n\n\nKey takeaways from summary statistics:\n\nFinal grades G3 range from 0 to 20, with an average of around 10.26.\nThe average study time is about 2.05 hours.\nMost students have zero reported failures.\nAlcohol consumption (Dalc and Walc) and socializing habits (goout) appear to vary across the student population.\n\nLet’s create a visualization to explore the final grades G3 distribution. We will use a histogram as it allows us to see the spread.\n\n# Visualization of grade distributions\neda_plot1 = alt.Chart(train_df).mark_bar().encode(\n    x=alt.X('G3:Q', bin=True, title='Final Grades (G3)'),\n    y=alt.Y('count()', title='Number of Students'),\n    tooltip=['G3']\n).properties(\n    title='Distribution of Final Grades (G3)',\n    width=400,\n    height=200\n)\neda_plot1 \n\n&lt;VegaLite 5 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/display_frontends.html#troubleshooting\n\n\nFigure 4: Distribution of Final Grades (G3)\nThe histogram shows that most students achieve grades between 8 and 15, with fewer students scoring very low or very high.\n\n# ally.dist(train_df).properties(title=\"Density Plot for all numeric columns\")\nfig, axes = plt.subplots(3, 3, figsize=(8, 8), sharey=False, sharex=False)\naxes = axes.flatten()\nnumeric_columns = train_df.select_dtypes(include='number').columns\nfor i, column in enumerate(numeric_columns):\n    dp = sns.kdeplot(data=train_df, x=column, fill=True, ax=axes[i])\nplt.tight_layout()\n\n\n\n\n\n\n\n\nFigure 5: Density plot for each numeric columns (including the target G3)\nSome interesting observations:\n\nThe distirbution of the grades G3, G2, G1 are somewhat bell-shaped.\nMost student do not consume alcohol, or very minimally.\nMost student studies around 2-5 hours a week and most of them also did not fail any previous classes.\n\n\n# ally.corr(train_df).properties(title=\"Correlation matrices for each numeric column pair\")\ncorr_mat = train_df.select_dtypes(include='number').corr() \\\n    .reset_index(names=\"var1\") \\\n    .melt(id_vars=\"var1\", var_name=\"var2\", value_name=\"correlation\")\n# get rid of \"duplicated\" correlation\ncorr_mat = corr_mat[corr_mat['var1'] &lt;= corr_mat['var2']].reset_index(drop=True)\ncorr_mat[\"abs_corr\"] = np.abs(corr_mat[\"correlation\"])\nalt.Chart(corr_mat).mark_circle().encode(\n    alt.X(\"var1\").title(\"variable 1\"),\n    alt.Y(\"var2\").title(\"variable 2\"),\n    alt.Color(\"correlation\").scale(domain=[-1, 1], scheme=\"blueorange\"),\n    alt.Size(\"abs_corr\").legend(None)\n).properties(\n    width=250,\n    height=250,\n    title=\"Pairwise correlations between variables (including target)\"\n)\n\n&lt;VegaLite 5 object&gt;\n\nIf you see this message, it means the renderer has not been properly enabled\nfor the frontend that you are using. For more information, see\nhttps://altair-viz.github.io/user_guide/display_frontends.html#troubleshooting\n\n\nFigure 6: Correlation matrices for each numeric columns (including target G3)\nSome interesting observations:\n\nThe grades are very correlated with one another\nAlcohol consumptions are somewhat negatively correlated with grades\nStudy time are somewhat positively correlated with grades/\n\n\n\n\n3.3 Analysis\n\n# Split features and target\nX_train, y_train = (\n    train_df.drop(columns=['G3']),\n    train_df['G3']\n)\nX_test, y_test = (\n    test_df.drop(columns=['G3']),\n    test_df['G3']\n)\n\n\nX_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 316 entries, 288 to 365\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   sex        316 non-null    object\n 1   age        316 non-null    int64 \n 2   studytime  316 non-null    int64 \n 3   failures   316 non-null    int64 \n 4   goout      316 non-null    int64 \n 5   Dalc       316 non-null    int64 \n 6   Walc       316 non-null    int64 \n 7   G1         316 non-null    int64 \n 8   G2         316 non-null    int64 \ndtypes: int64(8), object(1)\nmemory usage: 24.7+ KB\n\n\n\n\n3.4 Baseline Model\n\ndr = DummyRegressor()\n\ndummy_cv = cross_validate(dr, X_train, y_train, return_train_score=True)\npd.DataFrame(dummy_cv).agg(['mean']).T\n\n\n\n\n\n\n\n\nmean\n\n\n\n\nfit_time\n0.000539\n\n\nscore_time\n0.000369\n\n\ntest_score\n-0.006492\n\n\ntrain_score\n0.000000\n\n\n\n\n\n\n\n\n\n3.5 Define categorical and numerical columns\n\ncategorical_feats = X_train.select_dtypes(include=['object']).columns\nnumeric_feats = X_train.select_dtypes(include=['int64']).columns\n\n\n# Apply column transformers\npreprocessor = make_column_transformer(    \n    (StandardScaler(), numeric_feats),  # scaling on numeric features \n    (OneHotEncoder(drop=\"if_binary\"), categorical_feats),  # OHE on categorical features\n)\n\n#  Make pipeline\npipe_lr = make_pipeline(preprocessor, Ridge())\n\n\n# Define parameter grid\nparam_grid = {\n    'ridge__alpha': [0.1, 1, 10, 100]\n}\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(pipe_lr, param_grid=param_grid, n_jobs=-1, return_train_score=True)\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('standardscaler',\n                                                                         StandardScaler(),\n                                                                         Index(['age', 'studytime', 'failures', 'goout', 'Dalc', 'Walc', 'G1', 'G2'], dtype='object')),\n                                                                        ('onehotencoder',\n                                                                         OneHotEncoder(drop='if_binary'),\n                                                                         Index(['sex'], dtype='object'))])),\n                                       ('ridge', Ridge())]),\n             n_jobs=-1, param_grid={'ridge__alpha': [0.1, 1, 10, 100]},\n             return_train_score=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('standardscaler',\n                                                                         StandardScaler(),\n                                                                         Index(['age', 'studytime', 'failures', 'goout', 'Dalc', 'Walc', 'G1', 'G2'], dtype='object')),\n                                                                        ('onehotencoder',\n                                                                         OneHotEncoder(drop='if_binary'),\n                                                                         Index(['sex'], dtype='object'))])),\n                                       ('ridge', Ridge())]),\n             n_jobs=-1, param_grid={'ridge__alpha': [0.1, 1, 10, 100]},\n             return_train_score=True) best_estimator_: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  Index(['age', 'studytime', 'failures', 'goout', 'Dalc', 'Walc', 'G1', 'G2'], dtype='object')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(drop='if_binary'),\n                                                  Index(['sex'], dtype='object'))])),\n                ('ridge', Ridge(alpha=1))])  columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 Index(['age', 'studytime', 'failures', 'goout', 'Dalc', 'Walc', 'G1', 'G2'], dtype='object')),\n                                ('onehotencoder',\n                                 OneHotEncoder(drop='if_binary'),\n                                 Index(['sex'], dtype='object'))]) standardscalerIndex(['age', 'studytime', 'failures', 'goout', 'Dalc', 'Walc', 'G1', 'G2'], dtype='object')  StandardScaler?Documentation for StandardScalerStandardScaler() onehotencoderIndex(['sex'], dtype='object')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(drop='if_binary')  Ridge?Documentation for RidgeRidge(alpha=1) \n\n\n\n# Best score\ngrid_search.best_score_\n\nnp.float64(0.8097283181869402)\n\n\n\n# Get the best hyperparameter value\ngrid_search.best_params_\n\n{'ridge__alpha': 1}\n\n\n\n# Define the best model\nbest_model = grid_search.best_estimator_\n\n\npd.DataFrame(grid_search.cv_results_)[\n    [\n        \"mean_test_score\",\n        \"param_ridge__alpha\",\n        \"mean_fit_time\",\n        \"rank_test_score\",\n    ]\n].set_index(\"rank_test_score\").sort_index().T\n\n\n\n\n\n\n\nrank_test_score\n1\n2\n3\n4\n\n\n\n\nmean_test_score\n0.809728\n0.809702\n0.808180\n0.765174\n\n\nparam_ridge__alpha\n1.000000\n0.100000\n10.000000\n100.000000\n\n\nmean_fit_time\n0.044273\n0.059106\n0.014834\n0.007603\n\n\n\n\n\n\n\n\n# Apply best model on test set\ny_pred = best_model.predict(X_test)\n\n\n# Create a dataframe to compare observed and predicted values\ncomparison = pd.DataFrame({\n    \"Observed (y_test)\": y_test.values,\n    \"Predicted (y_pred)\": y_pred\n})\ncomparison.head(10)\n\n\n\n\n\n\n\n\nObserved (y_test)\nPredicted (y_pred)\n\n\n\n\n0\n8\n8.253681\n\n\n1\n13\n12.963188\n\n\n2\n12\n11.922506\n\n\n3\n0\n5.186794\n\n\n4\n10\n9.629068\n\n\n5\n12\n9.214681\n\n\n6\n5\n3.293215\n\n\n7\n0\n4.514123\n\n\n8\n16\n14.885317\n\n\n9\n13\n11.746364\n\n\n\n\n\n\n\n\n\n3.6 Model Evaluation\nThe Table 1 below summarizes the performance metrics of the model on the test dataset. These metrics help us evaluate the model’s ability to generalize to unseen data.\n\n\n\n\nTable 1: Performance metrics on test data\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nMean Squared Error (MSE)\n4.04828\n\n\nRoot Mean Squared Error (RMSE)\n2.01203\n\n\nMean Absolute Error (MAE)\n1.3086\n\n\n\n\n\n\n\n\nNext, we analyze the coefficients of the Ridge regression model. The Table 2 shows the values of the coefficients, which indicate the importance of each feature in predicting the target variable.\n\n\n\n\nTable 2: Coefficients of Ridge model\n\n\n\n\n\n\nfeatures\ncoefs\n\n\n\n\nage\n-0.351724\n\n\nstudytime\n-0.129035\n\n\nfailures\n0.101029\n\n\ngoout\n0.225087\n\n\nDalc\n0.00580251\n\n\nWalc\n0.0437156\n\n\nG1\n0.539314\n\n\nG2\n3.65733\n\n\nsex_M\n0.0262267\n\n\n\n\n\n\n\n\nThe following Figure 4 visualizes the coefficients of the Ridge regression model. Features with higher absolute coefficients have more impact on the model’s predictions.\n\n\n\n\n\n\nFigure 4: Ridge regression coefficients."
  },
  {
    "objectID": "notebooks/report.html#results-discussion",
    "href": "notebooks/report.html#results-discussion",
    "title": "Predicting academic performance using demographic and behavioral Data",
    "section": "4 Results & Discussion",
    "text": "4 Results & Discussion\nThe Ridge Regression model, with tuned hyperparameters, demonstrated well predictive capabilities on student’s math performance. The optimal hyperparameter for Ridge was found to be alpha = 1, and the best cross-validation score is approximately 0.81. This indicates a strong predictive accuracy during the model’s validation phase.\nThe Ridge coefficients suggests that student performance is most strongly influenced by prior grades, with G2 having the greatest positive impact, followed by G1. Social behaviors like going out and weekend alcohol consumption also show a smaller positive influence, while age, study time, and workday alcohol consumption have a negative effect. Failures and gender appear to have extremely minimal influence on the final grade.\nBased on the evaluation on the test set, the model achieved the following performance metrics:\n\nMean Squared Error (MSE): 4.048\nRoot Mean Squared Error (RMSE): 2.012\nMean Absolute Error (MAE): 1.309\n\nThese metrics suggest that the model is reasonably accurate in predicting students’ final grades. However, there are areas for improvement. We can explore other models which could better capture the non-linear relationships and feature interactions. Another improvement we can do is to provide confidence intervals for predictions. This approach could enhance the reliability and interpretability of predictions and help readers make more informed decisions."
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to our project",
    "section": "",
    "text": "Contributing to our project\nThis outlines how to propose a change to this project.\n\nFixing typos\nSmall typos or grammatical errors in documentation may be edited directly using the GitHub web interface, so long as the changes are made in the source file.\n\nYES: you edit .py, .ipynb, .md files.\nNO: you edit an .html file, or any other files not listed in YES.\n\n\n\nPrerequisites\nBefore you make a substantial pull request, you should always file an issue and make sure someone from the team agrees that it’s a problem. If you’ve found a bug, create an associated issue and illustrate the bug with a minimal reproducible example (MRE).\nA reproducible example should include the following:\n\nClear Description: Explain what the issue or feature is.\nMinimal Code: Provide a minimal piece of code that demonstrates the problem or change.\nExpected Output: Specify what you expect the output to be.\nActual Output: Describe what actually happens when you run the code.\n\nHere’s an example of a MRE\n# Example of a minimal code to demonstrate the issue/feature\nimport pandas as pd\n\n# Create a simple DataFrame\ndata = {'Name': ['Alice', 'Bob', 'Charlie'],\n        'Math': [90, 85, 92],\n        'Science': [88, 79, 94]}\n\ndf = pd.DataFrame(data)\n\n# Example operation that causes the issue\ndf['Average'] = df[['Math', 'Science']].mean(axis=1)\n\n# Expected output: DataFrame with 'Average' column calculated\nprint(df)\nWhat to include:\n\nIssue Example: If the code has a bug, describe the issue. For example, “The calculation for ‘Average’ is incorrect due to missing values.”\nExpected Behavior: For example, “The ‘Average’ column should be computed correctly even with missing data.”\n\n\n\nPull request process\n\nPlease create a Git branch for each pull request (PR).\nProvide a descriptive title for your PR and include any relevant information in the PR description:\n\nWhat is the purpose of the change?\nWhat issue does it address (if applicable)?\nAny additional notes for the reviewers (e.g., tests added, documentation updated)\n\nNew code should follow the NumpyDoc style guide or PEP8 style guide.\n\n\n\nCode of Conduct\nPlease note that this project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms.\n\n\nAttribution\nThese contributing guidelines were adapted from the Breast Cancer Predictor Project contributing guidelines."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting academic performance using demographic and behavioral Data",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n\n\nExamples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n\n\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by sending an email to nhantien@student.ubc.ca. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership.\n\n\n\nThis Code of Conduct is copied from the UBC DSCI 100 Code of Conduct, which was adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-responsibilities",
    "href": "CODE_OF_CONDUCT.html#our-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by sending an email to nhantien@student.ubc.ca. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is copied from the UBC DSCI 100 Code of Conduct, which was adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Predicting academic performance using demographic and behavioral Data",
    "section": "",
    "text": "MIT License\nCopyright (c) 2024 Master of Data Science at the University of British Columbia\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "notebooks/student_performance_predictor_report.html",
    "href": "notebooks/student_performance_predictor_report.html",
    "title": "Predicting academic performance using demographic and behavioral Data",
    "section": "",
    "text": "by Zhengling Jiang, Colombe Tolokin, Franklin Aryee, Tien Nguyen\nPackages:\nimport pandas as pd\nimport altair as alt\n# import altair_ally as ally\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport pandera as pa\nfrom scipy.stats import shapiro\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.model_selection import cross_validate\n\nalt.renderers.enable(\"mimetype\")\n%matplotlib inline"
  },
  {
    "objectID": "notebooks/student_performance_predictor_report.html#summary",
    "href": "notebooks/student_performance_predictor_report.html#summary",
    "title": "Predicting academic performance using demographic and behavioral Data",
    "section": "Summary",
    "text": "Summary\nThis project investigates whether a student’s mathematics performance can be predicted using demographic and behavioral data, aiming to help educators in supporting students and tailoring educational strategies. Using a Ridge Regression model with optimized hyperparameters (alpha = 1), we achieved a strong predictive accuracy with a cross-validation score of 0.81 and evaluation metrics on the test set including an MSE of 3.83, RMSE of 1.96, and MAE of 1.27. While the model demonstrates robust performance, future work could explore non-linear models and provide confidence intervals for predictions to enhance interpretability and reliability, ultimately contributing to better educational outcomes."
  },
  {
    "objectID": "notebooks/student_performance_predictor_report.html#introduction",
    "href": "notebooks/student_performance_predictor_report.html#introduction",
    "title": "Predicting academic performance using demographic and behavioral Data",
    "section": "Introduction",
    "text": "Introduction\nMath teaches us to think logically and it also provides us with analytical and problem-solving skills. These skills can be applied to various academic and professional fields. However, student performance in mathematics can be influenced by many factors, like individual factor, social factor, and family factor. Research has shown that attributes such as study habits, age, social behaviour (alcohol consumptions, etc) and family background can significantly impact a student’s academic success (Amuda, Bulus, and Joseph 2016); Modi 2023; Hjarnaa et al. 2023). Understanding these factors is crucial for improving educational outcomes.\nIn this study, we aim to address this question: “Can we predict a student’s math academic performance based on the demographic and behavioral data?”. Answering this question is important because understanding the factors behind student performance can help teachers provide support to struggling students. Furthermore, the ability to predict academic performance could assist schools in developing educational strategies based on different backgrounds of students. The goal of this study is to develop a machine learning model capable of predicting student’s math performance with high accuracy.\nThe dataset used in this study contains detailed records of student demographics and behaviors, such as age, study habits, social behaviors, and family background. The target variable, mathematics performance, is measured as a continuous score reflecting students’ final grade. This dataset offers a great opportunity to explore meaningful relationships between features and academic outcomes."
  },
  {
    "objectID": "notebooks/student_performance_predictor_report.html#methods-results",
    "href": "notebooks/student_performance_predictor_report.html#methods-results",
    "title": "Predicting academic performance using demographic and behavioral Data",
    "section": "Methods & Results",
    "text": "Methods & Results\nThe objective here to prepare the data for our classification analysis by exploring relevant features and summarizing key insights through data wrangling and visualization.\n\nDataset Description\nThe full data set contains the following columns:\n\nschool - student’s school (binary: ‘GP’ - Gabriel Pereira or ‘MS’ - Mousinho da Silveira)\nsex - student’s sex (binary: ‘F’ - female or ‘M’ - male)\nage - student’s age (numeric: from 15 to 22)\naddress - student’s home address type (binary: ‘U’ - urban or ‘R’ - rural)\nfamsize - family size (binary: ‘LE3’ - less or equal to 3 or ‘GT3’ - greater than 3)\nPstatus - parent’s cohabitation status (binary: ‘T’ - living together or ‘A’ - apart)\nMedu - mother’s education (numeric: 0 - none, 1 - primary education (4th grade), 2 - “ 5th to 9th grade, 3 - “ secondary education or 4 - “ higher education)\nFedu - father’s education (numeric: 0 - none, 1 - primary education (4th grade), 2 - “ 5th to 9th grade, 3 - “ secondary education or 4 - “ higher education)\nMjob - mother’s job (nominal: ‘teacher’, ‘health’ care related, civil ‘services’ (e.g. administrative or police), ‘at_home’ or ‘other’)\nFjob - father’s job (nominal: ‘teacher’, ‘health’ care related, civil ‘services’ (e.g. administrative or police), ‘at_home’ or ‘other’)\nreason - reason to choose this school (nominal: close to ‘home’, school ‘reputation’, ‘course’ preference or ‘other’)\nguardian - student’s guardian (nominal: ‘mother’, ‘father’ or ‘other’)\ntraveltime - home to school travel time (numeric: 1 - &lt;15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - &gt;1 hour)\nstudytime - weekly study time (numeric: 1 - &lt;2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - &gt;10 hours)\nfailures - number of past class failures (numeric: n if 1&lt;=n&lt;3, else 4)\nschoolsup - extra educational support (binary: yes or no)\nfamsup` - family educational support (binary: yes or no)\npaid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\nactivities - extra-curricular activities (binary: yes or no)\nnursery - attended nursery school (binary: yes or no)\nhigher - wants to take higher education (binary: yes or no)\ninternet - Internet access at home (binary: yes or no)\nromantic - with a romantic relationship (binary: yes or no)\nfamrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\nfreetime - free time after school (numeric: from 1 - very low to 5 - very high)\ngoout - going out with friends (numeric: from 1 - very low to 5 - very high)\nDalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\nWalc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\nhealth - current health status (numeric: from 1 - very bad to 5 - very good)\nabsences - number of school absences (numeric: from 0 to 93)\n\nThese columns represent the grades:\n\nG1 - first period grade (numeric: from 0 to 20)\nG2 - second period grade (numeric: from 0 to 20)\nG3 - final grade (numeric: from 0 to 20, output target)\n\nAttribution: The dataset variable description is copied as original from the UCI Machine Learning Repository.\n\n\nData Loading, Wrangling and Summary\nLet’s start by loading the data and have an initial view of data set structure.\nThe file is a .csv file with ; as delimiter. Let’s use pandasto read it in.\n\n!python ../src/download_data.py\n\nFile already existed, exitting script...\n\n\n\n# Validate and load data file format\ndef load_data(filename: str) -&gt; pd.DataFrame:      \n    if os.path.isfile(filename):\n        if filename[-4:] == '.csv':\n            df = pd.read_csv(filename, delimiter=';')\n            return df\n    return \"File is not in directory\"\n\nstudent_performance = load_data('../data/raw/student-mat.csv')\n\nThis provides an overview of the data set with 33 columns, each representing student attributes such as age, gender, study time, grades, and parental details.\nLet’s get some information on the data set to better understand it.\n\nstudent_performance.head()\n\n\n\n\n\n\n\n\nschool\nsex\nage\naddress\nfamsize\nPstatus\nMedu\nFedu\nMjob\nFjob\n...\nfamrel\nfreetime\ngoout\nDalc\nWalc\nhealth\nabsences\nG1\nG2\nG3\n\n\n\n\n0\nGP\nF\n18\nU\nGT3\nA\n4\n4\nat_home\nteacher\n...\n4\n3\n4\n1\n1\n3\n6\n5\n6\n6\n\n\n1\nGP\nF\n17\nU\nGT3\nT\n1\n1\nat_home\nother\n...\n5\n3\n3\n1\n1\n3\n4\n5\n5\n6\n\n\n2\nGP\nF\n15\nU\nLE3\nT\n1\n1\nat_home\nother\n...\n4\n3\n2\n2\n3\n3\n10\n7\n8\n10\n\n\n3\nGP\nF\n15\nU\nGT3\nT\n4\n2\nhealth\nservices\n...\n3\n2\n2\n1\n1\n5\n2\n15\n14\n15\n\n\n4\nGP\nF\n16\nU\nGT3\nT\n3\n3\nother\nother\n...\n4\n3\n2\n1\n2\n5\n4\n6\n10\n10\n\n\n\n\n5 rows × 33 columns\n\n\n\n\nstudent_performance.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 395 entries, 0 to 394\nData columns (total 33 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   school      395 non-null    object\n 1   sex         395 non-null    object\n 2   age         395 non-null    int64 \n 3   address     395 non-null    object\n 4   famsize     395 non-null    object\n 5   Pstatus     395 non-null    object\n 6   Medu        395 non-null    int64 \n 7   Fedu        395 non-null    int64 \n 8   Mjob        395 non-null    object\n 9   Fjob        395 non-null    object\n 10  reason      395 non-null    object\n 11  guardian    395 non-null    object\n 12  traveltime  395 non-null    int64 \n 13  studytime   395 non-null    int64 \n 14  failures    395 non-null    int64 \n 15  schoolsup   395 non-null    object\n 16  famsup      395 non-null    object\n 17  paid        395 non-null    object\n 18  activities  395 non-null    object\n 19  nursery     395 non-null    object\n 20  higher      395 non-null    object\n 21  internet    395 non-null    object\n 22  romantic    395 non-null    object\n 23  famrel      395 non-null    int64 \n 24  freetime    395 non-null    int64 \n 25  goout       395 non-null    int64 \n 26  Dalc        395 non-null    int64 \n 27  Walc        395 non-null    int64 \n 28  health      395 non-null    int64 \n 29  absences    395 non-null    int64 \n 30  G1          395 non-null    int64 \n 31  G2          395 non-null    int64 \n 32  G3          395 non-null    int64 \ndtypes: int64(16), object(17)\nmemory usage: 102.0+ KB\n\n\nThe data set contains 395 observations and 33 columns covering different aspects of student demographics, academic and behavioral traits.\nWe can see that there is no missing values. There is not need to handle NAs.\nThe data set includes categorical (school, sex, Mjob) and numerical (age, G1, G2, G3) features.\nThere is a large range of features but not all of them are necessary for this analysis. Let’s proceed and select only the necessary ones.\nLet’s selected the following key columns:\n\nDemographic attributes: sex, age\nAcademic Attributes: studytime, failures, G1, G2, G3 (grades for three terms)\nBehavioral Attributes: goout (socializing), Dalc (weekday alcohol consumption), Walc (weekend alcohol consumption)\n\nWe will also split the dataset into train and test set with a 80/20 ratio. We also set random_state=123 for reproducibility.\n\n# Necessary columns\ncolumns = ['sex', \n           'age', \n           'studytime', \n           'failures', \n           'goout', \n           'Dalc', \n           'Walc',\n           'G1',\n           'G2',\n           'G3']\n\nsubset_df = student_performance[columns]\n\ntrain_df, test_df = train_test_split(\n    subset_df, test_size=0.2, random_state=123\n)\n\n\nData Validation Checks\n\n# Data validation checks:\n\n# Correct column names\n# No empty observations\n# Correct data types in each column\n# No duplicate observations\n# Correct category levels (i.e., no string mismatches or single values)\nschema = pa.DataFrameSchema(\n    {\n        \"sex\": pa.Column(str, pa.Check.isin([\"M\", \"F\"])),\n        \"age\": pa.Column(int, pa.Check.between(15, 22), nullable=False),\n        \"studytime\": pa.Column(int, pa.Check.between(1, 4), nullable=False), \n        \"failures\": pa.Column(int, pa.Check.between(0, 4), nullable=False),\n        \"goout\": pa.Column(int, pa.Check.between(1, 5), nullable=False),\n        \"Dalc\": pa.Column(int, pa.Check.between(1, 5), nullable=False),\n        \"Walc\": pa.Column(int, pa.Check.between(1, 5), nullable=False),\n        \"G1\": pa.Column(int, pa.Check.between(0, 20), nullable=False),\n        \"G2\": pa.Column(int, pa.Check.between(0, 20), nullable=False),\n        \"G3\": pa.Column(int, pa.Check.between(0, 20), nullable=False)\n    },\n    checks=[\n        pa.Check(lambda df: ~df.duplicated().any(), error=\"Duplicate rows found.\"),\n        pa.Check(lambda df: ~(df.isna().all(axis=1)).any(), error=\"Empty rows found.\")\n    ]\n)\n\nschema.validate(subset_df, lazy=True);\n\n\n# Missingness Not Beyond Expected Threshold\ndef validate_missingness(data: pd.DataFrame, threshold: float = 0.05) -&gt; None:\n    \"\"\"\n    Validate that missing values in the dataset do not exceed the acceptable threshold and visualize the missingness.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The dataset to check for missing values.\n    threshold : float, optional\n        The maximum allowable percentage of missing values per column (default is 0.05).\n\n    Returns\n    -------\n    None\n        This function does not return anything. It prints a message indicating whether missing values are within\n        the acceptable threshold and displays a heatmap of missing values in the dataset.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; df = pd.DataFrame({\n    &gt;&gt;&gt;     'A': [1, 2, None, 4, 5],\n    &gt;&gt;&gt;     'B': [None, None, 3, 4, 5],\n    &gt;&gt;&gt;     'C': [1, 2, 3, 4, 5]\n    &gt;&gt;&gt; })\n    &gt;&gt;&gt; validate_missingness(df, threshold=0.1)\n    \"\"\"\n    \n    print(\"Validating missingness...\")\n    missing_percentage = data.isnull().mean()\n    above_threshold = missing_percentage[missing_percentage &gt; threshold]\n\n    if above_threshold.empty:\n        print(\"Missingness is within the acceptable threshold.\")\n    else:\n        print(f\"Columns with missing values beyond threshold ({threshold}):\")\n        print(above_threshold)\n\n    # To visualize missingness\n    plt.figure(figsize=(10, 6))\n    sns.heatmap(\n        data.isnull(),\n        cbar=True,\n        cmap=\"viridis\",\n        cbar_kws={'label': 'Missing Value Indicator (1 = Missing, 0 = Present)'}\n    )\n    plt.title(\"Missing Value Heatmap\")\n    plt.show()\nvalidate_missingness(subset_df)\n\nValidating missingness...\nMissingness is within the acceptable threshold.\n\n\n\n\n\n\n\n\n\nFigure 1: Missing Values Heatmap\n\n# Target/Response Variable Follows Expected Distribution\ndef validate_target_distribution(data: pd.DataFrame, target_column: str) -&gt; None:\n    \"\"\"\n    Validate the distribution of the target variable by plotting its histogram and performing a Shapiro-Wilk test.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The dataset containing the target variable.\n    target_column : str\n        The name of the target column whose distribution is to be validated.\n\n    Returns\n    -------\n    None\n        This function does not return anything. It displays a histogram with a KDE of the target variable\n        and prints the result of the normality test.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; df = pd.DataFrame({\n    &gt;&gt;&gt;     'target': [1.2, 2.3, 2.9, 3.5, 2.1, 3.8, 4.0, 5.1]\n    &gt;&gt;&gt; })\n    &gt;&gt;&gt; validate_target_distribution(df, 'target')\n    \"\"\"\n    print(\"Validating target distribution...\")\n    sns.histplot(data[target_column], \n                 kde=True, \n                 bins=20)\n    plt.title(f\"Distribution of {target_column}\")\n    plt.show()\n    stat, p = shapiro(data[target_column])\n    if p &gt; 0.05:\n        print(f\"Target variable '{target_column}' follows a normal distribution (p={p:.4f}).\")\n    else:\n        print(f\"Target variable '{target_column}' does not follow a normal distribution (p={p:.4f}).\")\nvalidate_target_distribution(subset_df, target_column=\"G3\")\n\nValidating target distribution...\n\n\n\n\n\n\n\n\n\nTarget variable 'G3' does not follow a normal distribution (p=0.0000).\n\n\nFigure 2: Distribution of the target variable\n\n\nChecking for Outliers\nBefore proceeding with preprocessing and modeling, we check for potential outliers or anomalous values in the training dataset.\n\ndef validate_no_outliers(data: pd.DataFrame, numeric_columns: list, max_cols: int = 3) -&gt; None:\n    \"\"\"\n    Validate the presence of outliers in numeric columns using boxplots.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The dataset containing the numeric columns to be checked for outliers.\n    numeric_columns : list\n        A list of column names from `data` that contain numeric data to plot.\n    max_cols : int, optional\n        The maximum number of boxplots to display per row (default is 3).\n\n    Returns\n    -------\n    None\n        This function does not return anything. It displays boxplots for the specified numeric columns.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; from matplotlib import pyplot as plt\n    &gt;&gt;&gt; import seaborn as sns\n    &gt;&gt;&gt; df = pd.DataFrame({\n    ...     'A': [1, 2, 3, 4, 100],\n    ...     'B': [10, 20, 30, 40, 50],\n    ...     'C': [5, 15, 25, 35, 45]\n    ... })\n    &gt;&gt;&gt; validate_no_outliers(data=df, numeric_columns=['A', 'B', 'C'], max_cols=2)\n    \"\"\"\n    print(\"Validating outliers...\")\n\n    num_plots = len(numeric_columns)\n    nrows = -(-num_plots // max_cols)\n    ncols = min(num_plots, max_cols)\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 10))\n\n    for ax, column in zip(axes.flatten(), numeric_columns):\n        sns.boxplot(data=subset_df, x=column, ax=ax)\n        ax.set_title(f\"Boxplot of {column}\")\n    \n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n    plt.suptitle(\"Boxplots of Numeric Columns\", fontsize=14)\n    plt.show()\n\n\n# checking outlier for dataset\nnumeric_columns = subset_df.select_dtypes(include='number').columns\n\nvalidate_no_outliers(subset_df, numeric_columns)\n\nValidating outliers...\n\n\n\n\n\n\n\n\n\nFigure 3: Outliers visualization\nThere are few outliers in failures, Dalc, age, studytime, G2, and G1, as seen in the boxplots. These outliers are relatively few compared to the 395 entries, but could still influence model results. We will apply a StandardScaler transformation to the numeric variables, the effect of these outliers will be minimized. Therefore, we will not drop or modify these outliers at this step.\n\nimport warnings\n\ndef validate_anomalous_correlations(data: pd.DataFrame, target_col: str, threshold: float = 0.9, zero_tolerance: float = 1e-5):\n    \"\"\"\n    Check for anomalous correlations in a dataset:\n    - Between features and the target variable.\n    - Among features themselves.\n    - Includes checks for zero correlations.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The dataset containing the target and features.\n    target_col : str\n        The name of the target column in the dataset.\n    threshold : float, optional\n        The correlation threshold above which correlations are flagged as anomalous (default is 0.9).\n    zero_tolerance : float, optional\n        The tolerance for detecting zero correlations (default is 1e-5).\n\n    Returns\n    -------\n    dict\n        A dictionary containing:\n        - 'feature_to_target': DataFrame of correlations between features and the target.\n        - 'feature_to_feature': DataFrame of correlations between features themselves.\n\n    Raises\n    ------\n    Warning\n        Issues warnings for high and near-zero correlations.\n    \"\"\"\n    if target_col not in data.columns:\n        raise ValueError(f\"Target column '{target_col}' not found in the dataset.\")\n    \n    features = data.drop(columns=[target_col])\n    target = data[target_col]\n\n    # Compute the full correlation matrix\n    correlation_matrix = data.corr()\n\n    # Step 1: Correlations between features and target\n    target_correlations = correlation_matrix[target_col].drop(target_col)\n\n    # Check for anomalous (high) correlations\n    anomalous_target_corrs = target_correlations[target_correlations.abs() &gt; threshold]\n    for feature, corr in anomalous_target_corrs.items():\n        warnings.warn(\n            f\"Anomalous correlation ({corr:.2f}) between feature '{feature}' and target '{target_col}'.\"\n        )\n\n    # Check for zero correlations\n    zero_target_corrs = target_correlations[target_correlations.abs() &lt;= zero_tolerance]\n    for feature, corr in zero_target_corrs.items():\n        warnings.warn(\n            f\"Zero or near-zero correlation ({corr:.2f}) between feature '{feature}' and target '{target_col}'.\"\n        )\n\n    # Step 2: Correlations among features\n    feature_correlations = correlation_matrix.loc[features.columns, features.columns]\n\n    # Check for anomalous (high) correlations among features\n    anomalous_feature_corrs = feature_correlations[\n        (feature_correlations.abs() &gt; threshold) & (feature_correlations != 1)\n    ]\n    for feature1 in anomalous_feature_corrs.index:\n        for feature2 in anomalous_feature_corrs.columns:\n            if feature1 != feature2 and not pd.isna(anomalous_feature_corrs.loc[feature1, feature2]):\n                corr = anomalous_feature_corrs.loc[feature1, feature2]\n                warnings.warn(\n                    f\"Anomalous correlation ({corr:.2f}) between features '{feature1}' and '{feature2}'.\"\n                )\n                anomalous_feature_corrs.loc[feature1, feature2] = None\n                anomalous_feature_corrs.loc[feature2, feature1] = None\n\n    feature_to_target_df = pd.DataFrame({\n        'Feature': target_correlations.index,\n        'Correlation': target_correlations.values\n    }).reset_index(drop=True)\n\n    feature_to_feature_df = feature_correlations.stack().reset_index()\n    feature_to_feature_df.columns = ['Feature1', 'Feature2', 'Correlation']\n    feature_to_feature_df = feature_to_feature_df[\n        feature_to_feature_df['Feature1'] != feature_to_feature_df['Feature2']\n    ]\n\n    return {\n        'feature_to_target': feature_to_target_df,\n        'feature_to_feature': feature_to_feature_df\n    }\n\n# a warning will be issue if there are any problems\ncorrelation_check = validate_anomalous_correlations(\n    train_df.select_dtypes(include=\"number\"), \"G3\", threshold=0.95\n)\n\n\ntrain_df.head()\n\n\n\n\n\n\n\n\nsex\nage\nstudytime\nfailures\ngoout\nDalc\nWalc\nG1\nG2\nG3\n\n\n\n\n288\nM\n18\n3\n0\n4\n1\n3\n15\n14\n14\n\n\n6\nM\n16\n2\n0\n4\n1\n1\n12\n12\n11\n\n\n226\nF\n17\n2\n0\n4\n1\n3\n16\n15\n15\n\n\n319\nF\n18\n2\n0\n4\n3\n3\n11\n11\n11\n\n\n216\nF\n17\n2\n2\n5\n2\n4\n6\n6\n4\n\n\n\n\n\n\n\n\ntrain_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 316 entries, 288 to 365\nData columns (total 10 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   sex        316 non-null    object\n 1   age        316 non-null    int64 \n 2   studytime  316 non-null    int64 \n 3   failures   316 non-null    int64 \n 4   goout      316 non-null    int64 \n 5   Dalc       316 non-null    int64 \n 6   Walc       316 non-null    int64 \n 7   G1         316 non-null    int64 \n 8   G2         316 non-null    int64 \n 9   G3         316 non-null    int64 \ndtypes: int64(9), object(1)\nmemory usage: 27.2+ KB\n\n\nLet’s get a summary of the training set we are going to use for the analysis.\n\ntrain_df.describe()\n\n\n\n\n\n\n\n\nage\nstudytime\nfailures\ngoout\nDalc\nWalc\nG1\nG2\nG3\n\n\n\n\ncount\n316.000000\n316.000000\n316.000000\n316.000000\n316.000000\n316.000000\n316.000000\n316.000000\n316.000000\n\n\nmean\n16.756329\n2.050633\n0.360759\n3.098101\n1.471519\n2.306962\n10.835443\n10.601266\n10.262658\n\n\nstd\n1.290056\n0.860398\n0.770227\n1.118330\n0.855874\n1.258904\n3.252078\n3.756797\n4.522676\n\n\nmin\n15.000000\n1.000000\n0.000000\n1.000000\n1.000000\n1.000000\n4.000000\n0.000000\n0.000000\n\n\n25%\n16.000000\n1.000000\n0.000000\n2.000000\n1.000000\n1.000000\n8.000000\n8.750000\n8.000000\n\n\n50%\n17.000000\n2.000000\n0.000000\n3.000000\n1.000000\n2.000000\n11.000000\n11.000000\n11.000000\n\n\n75%\n18.000000\n2.000000\n0.000000\n4.000000\n2.000000\n3.000000\n13.000000\n13.000000\n13.000000\n\n\nmax\n22.000000\n4.000000\n3.000000\n5.000000\n5.000000\n5.000000\n19.000000\n19.000000\n20.000000\n\n\n\n\n\n\n\nKey takeaways from summary statistics:\n\nFinal grades G3 range from 0 to 20, with an average of around 10.26.\nThe average study time is about 2.05 hours.\nMost students have zero reported failures.\nAlcohol consumption (Dalc and Walc) and socializing habits (goout) appear to vary across the student population.\n\nLet’s create a visualization to explore the final grades G3 distribution. We will use a histogram as it allows us to see the spread.\n\n# Visualization of grade distributions\neda_plot1 = alt.Chart(train_df).mark_bar().encode(\n    x=alt.X('G3:Q', bin=True, title='Final Grades (G3)'),\n    y=alt.Y('count()', title='Number of Students'),\n    tooltip=['G3']\n).properties(\n    title='Distribution of Final Grades (G3)',\n    width=400,\n    height=200\n)\neda_plot1 \n\n\n\n\n\n\n\n\nFigure 4: Distribution of Final Grades (G3)\nThe histogram shows that most students achieve grades between 8 and 15, with fewer students scoring very low or very high.\n\n# ally.dist(train_df).properties(title=\"Density Plot for all numeric columns\")\nfig, axes = plt.subplots(3, 3, figsize=(8, 8), sharey=False, sharex=False)\naxes = axes.flatten()\nnumeric_columns = train_df.select_dtypes(include='number').columns\nfor i, column in enumerate(numeric_columns):\n    dp = sns.kdeplot(data=train_df, x=column, fill=True, ax=axes[i])\nplt.tight_layout()\n\n\n\n\n\n\n\n\nFigure 5: Density plot for each numeric columns (including the target G3)\nSome interesting observations:\n\nThe distirbution of the grades G3, G2, G1 are somewhat bell-shaped.\nMost student do not consume alcohol, or very minimally.\nMost student studies around 2-5 hours a week and most of them also did not fail any previous classes.\n\n\n# ally.corr(train_df).properties(title=\"Correlation matrices for each numeric column pair\")\ncorr_mat = train_df.select_dtypes(include='number').corr() \\\n    .reset_index(names=\"var1\") \\\n    .melt(id_vars=\"var1\", var_name=\"var2\", value_name=\"correlation\")\n# get rid of \"duplicated\" correlation\ncorr_mat = corr_mat[corr_mat['var1'] &lt;= corr_mat['var2']].reset_index(drop=True)\ncorr_mat[\"abs_corr\"] = np.abs(corr_mat[\"correlation\"])\nalt.Chart(corr_mat).mark_circle().encode(\n    alt.X(\"var1\").title(\"variable 1\"),\n    alt.Y(\"var2\").title(\"variable 2\"),\n    alt.Color(\"correlation\").scale(domain=[-1, 1], scheme=\"blueorange\"),\n    alt.Size(\"abs_corr\").legend(None)\n).properties(\n    width=250,\n    height=250,\n    title=\"Pairwise correlations between variables (including target)\"\n)\n\n\n\n\n\n\n\n\nFigure 6: Correlation matrices for each numeric columns (including target G3)\nSome interesting observations:\n\nThe grades are very correlated with one another\nAlcohol consumptions are somewhat negatively correlated with grades\nStudy time are somewhat positively correlated with grades/\n\n\n\n\nAnalysis\n\n# Split features and target\nX_train, y_train = (\n    train_df.drop(columns=['G3']),\n    train_df['G3']\n)\nX_test, y_test = (\n    test_df.drop(columns=['G3']),\n    test_df['G3']\n)\n\n\nX_train.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 316 entries, 288 to 365\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   sex        316 non-null    object\n 1   age        316 non-null    int64 \n 2   studytime  316 non-null    int64 \n 3   failures   316 non-null    int64 \n 4   goout      316 non-null    int64 \n 5   Dalc       316 non-null    int64 \n 6   Walc       316 non-null    int64 \n 7   G1         316 non-null    int64 \n 8   G2         316 non-null    int64 \ndtypes: int64(8), object(1)\nmemory usage: 24.7+ KB\n\n\n\n\nBaseline Model\n\ndr = DummyRegressor()\n\ndummy_cv = cross_validate(dr, X_train, y_train, return_train_score=True)\npd.DataFrame(dummy_cv).agg(['mean']).T\n\n\n\n\n\n\n\n\nmean\n\n\n\n\nfit_time\n0.001057\n\n\nscore_time\n0.000723\n\n\ntest_score\n-0.006492\n\n\ntrain_score\n0.000000\n\n\n\n\n\n\n\n\n\nDefine categorical and numerical columns\n\ncategorical_feats = X_train.select_dtypes(include=['object']).columns\nnumeric_feats = X_train.select_dtypes(include=['int64']).columns\n\n\n# Apply column transformers\npreprocessor = make_column_transformer(    \n    (StandardScaler(), numeric_feats),  # scaling on numeric features \n    (OneHotEncoder(drop=\"if_binary\"), categorical_feats),  # OHE on categorical features\n)\n\n#  Make pipeline\npipe_lr = make_pipeline(preprocessor, Ridge())\n\n\n# Define parameter grid\nparam_grid = {\n    'ridge__alpha': [0.1, 1, 10, 100]\n}\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(pipe_lr, param_grid=param_grid, n_jobs=-1, return_train_score=True)\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('standardscaler',\n                                                                         StandardScaler(),\n                                                                         Index(['age', 'studytime', 'failures', 'goout', 'Dalc', 'Walc', 'G1', 'G2'], dtype='object')),\n                                                                        ('onehotencoder',\n                                                                         OneHotEncoder(drop='if_binary'),\n                                                                         Index(['sex'], dtype='object'))])),\n                                       ('ridge', Ridge())]),\n             n_jobs=-1, param_grid={'ridge__alpha': [0.1, 1, 10, 100]},\n             return_train_score=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(estimator=Pipeline(steps=[('columntransformer',\n                                        ColumnTransformer(transformers=[('standardscaler',\n                                                                         StandardScaler(),\n                                                                         Index(['age', 'studytime', 'failures', 'goout', 'Dalc', 'Walc', 'G1', 'G2'], dtype='object')),\n                                                                        ('onehotencoder',\n                                                                         OneHotEncoder(drop='if_binary'),\n                                                                         Index(['sex'], dtype='object'))])),\n                                       ('ridge', Ridge())]),\n             n_jobs=-1, param_grid={'ridge__alpha': [0.1, 1, 10, 100]},\n             return_train_score=True) best_estimator_: PipelinePipeline(steps=[('columntransformer',\n                 ColumnTransformer(transformers=[('standardscaler',\n                                                  StandardScaler(),\n                                                  Index(['age', 'studytime', 'failures', 'goout', 'Dalc', 'Walc', 'G1', 'G2'], dtype='object')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(drop='if_binary'),\n                                                  Index(['sex'], dtype='object'))])),\n                ('ridge', Ridge(alpha=1))])  columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformerColumnTransformer(transformers=[('standardscaler', StandardScaler(),\n                                 Index(['age', 'studytime', 'failures', 'goout', 'Dalc', 'Walc', 'G1', 'G2'], dtype='object')),\n                                ('onehotencoder',\n                                 OneHotEncoder(drop='if_binary'),\n                                 Index(['sex'], dtype='object'))]) standardscalerIndex(['age', 'studytime', 'failures', 'goout', 'Dalc', 'Walc', 'G1', 'G2'], dtype='object')  StandardScaler?Documentation for StandardScalerStandardScaler() onehotencoderIndex(['sex'], dtype='object')  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(drop='if_binary')  Ridge?Documentation for RidgeRidge(alpha=1) \n\n\n\n# Best score\ngrid_search.best_score_\n\nnp.float64(0.8097283181869402)\n\n\n\n# Get the best hyperparameter value\ngrid_search.best_params_\n\n{'ridge__alpha': 1}\n\n\n\n# Define the best model\nbest_model = grid_search.best_estimator_\n\n\npd.DataFrame(grid_search.cv_results_)[\n    [\n        \"mean_test_score\",\n        \"param_ridge__alpha\",\n        \"mean_fit_time\",\n        \"rank_test_score\",\n    ]\n].set_index(\"rank_test_score\").sort_index().T\n\n\n\n\n\n\n\nrank_test_score\n1\n2\n3\n4\n\n\n\n\nmean_test_score\n0.809728\n0.809702\n0.808180\n0.765174\n\n\nparam_ridge__alpha\n1.000000\n0.100000\n10.000000\n100.000000\n\n\nmean_fit_time\n0.080046\n0.201773\n0.033149\n0.024826\n\n\n\n\n\n\n\n\n# Apply best model on test set\ny_pred = best_model.predict(X_test)\n\n\n# Create a dataframe to compare observed and predicted values\ncomparison = pd.DataFrame({\n    \"Observed (y_test)\": y_test.values,\n    \"Predicted (y_pred)\": y_pred\n})\ncomparison.head(10)\n\n\n\n\n\n\n\n\nObserved (y_test)\nPredicted (y_pred)\n\n\n\n\n0\n8\n8.253681\n\n\n1\n13\n12.963188\n\n\n2\n12\n11.922506\n\n\n3\n0\n5.186794\n\n\n4\n10\n9.629068\n\n\n5\n12\n9.214681\n\n\n6\n5\n3.293215\n\n\n7\n0\n4.514123\n\n\n8\n16\n14.885317\n\n\n9\n13\n11.746364\n\n\n\n\n\n\n\n\n# Evaluate performance on the test set\nmse = mean_squared_error(y_test, y_pred)    # Mean squared error\n\nrmse = np.sqrt(mse) # Root Mean Squared error\n\nmae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n\n\n# Create a data frame to display performance metrics\nmetrics = pd.DataFrame({\n    \"Metric\": [\"Mean Squared Error (MSE)\", \n               \"Root Mean Squared Error (RMSE)\", \n               \"Mean Absolute Error (MAE)\"],\n    \"Value\": [mse, rmse, mae]\n})\nmetrics.set_index('Metric')\n\n\n\n\n\n\n\n\nValue\n\n\nMetric\n\n\n\n\n\nMean Squared Error (MSE)\n3.828837\n\n\nRoot Mean Squared Error (RMSE)\n1.956741\n\n\nMean Absolute Error (MAE)\n1.272044\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\n# Assume coefficients are stored in a matrix `coef` (predictors x responses)\ncoefs = best_model.named_steps['ridge'].coef_\nfeature_names = best_model.named_steps['columntransformer']\\\n    .get_feature_names_out().tolist()\nfeature_names = [n.split(\"__\")[1] for n in feature_names]\n\ncoefs_df = pd.DataFrame({\"features\": feature_names, \"coefs\": coefs})\ncoefs_df\n\n\n\n\n\n\n\n\nfeatures\ncoefs\n\n\n\n\n0\nage\n-0.228591\n\n\n1\nstudytime\n-0.172796\n\n\n2\nfailures\n0.017550\n\n\n3\ngoout\n0.152798\n\n\n4\nDalc\n-0.058226\n\n\n5\nWalc\n0.109286\n\n\n6\nG1\n0.643607\n\n\n7\nG2\n3.549430\n\n\n8\nsex_M\n0.002157\n\n\n\n\n\n\n\n\nplt.bar(feature_names, coefs)\nplt.xlabel(\"Features\")\nplt.ylabel(\"Coefficient Value\")\nplt.title(\"Ridge Regression Coefficients\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7: Ridge regression coefficients"
  },
  {
    "objectID": "notebooks/student_performance_predictor_report.html#results-discussion",
    "href": "notebooks/student_performance_predictor_report.html#results-discussion",
    "title": "Predicting academic performance using demographic and behavioral Data",
    "section": "Results & Discussion",
    "text": "Results & Discussion\nThe Ridge Regression model, with tuned hyperparameters, demonstrated well predictive capabilities on student’s math performance. The optimal hyperparameter for Ridge was found to be alpha = 1, and the best cross-validation score is approximately 0.81. This indicates a strong predictive accuracy during the model’s validation phase.\nThe Ridge coefficients suggests that student performance is most strongly influenced by prior grades, with G2 having the greatest positive impact, followed by G1. Social behaviors like going out and weekend alcohol consumption also show a smaller positive influence, while age, study time, and workday alcohol consumption have a negative effect. Failures and gender appear to have extremely minimal influence on the final grade.\nBased on the evaluation on the test set, the model achieved the following performance metrics:\n\nMean Squared Error (MSE): 3.83\nRoot Mean Squared Error (RMSE): 1.96\nMean Absolute Error (MAE): 1.27\n\nThese metrics suggest that the model is reasonably accurate in predicting students’ final grades. However, there are areas for improvement. We can explore other models which could better capture the non-linear relationships and feature interactions. Another improvement we can do is to provide confidence intervals for predictions. This approach could enhance the reliability and interpretability of predictions and help readers make more informed decisions."
  },
  {
    "objectID": "notebooks/student_performance_predictor_report.html#references",
    "href": "notebooks/student_performance_predictor_report.html#references",
    "title": "Predicting academic performance using demographic and behavioral Data",
    "section": "References",
    "text": "References\n\nAmuda, Bitrus Glawala, Apagu Kidlindila Bulus, and Hamsatu Pur Joseph. “Marital Status and Age as Predictors of Academic Performance of Students of Colleges of Education in the Nort- Eastern Nigeria.” American Journal of Educational Research 4.12 (2016): 896-902.\nCortez, Paulo. “Student Performance.” UCI Machine Learning Repository, 2008, https://doi.org/10.24432/C5TG7T.\nHjarnaa, Louise, Sanne Pagh, Møller, Alberte Brix, Curtis, Ulrik, Becker, Ove, Andersen, Fartein Ask, Torvik, Janne Schurmann, Tolstrup. “Alcohol Intake and Academic Performance and Dropout in High School: A Prospective Cohort Study in 65,233 Adolescents”. Journal of Adolescent Health 73. 6(2023): 1083–1092.\nModi, Y. G. “The Impact of Stress on Academic Performance: Strategies for High School Students.” International Journal of Psychiatry, vol. 8, no. 5, 2023, pp. 150–152."
  }
]