{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Predicting academic performance using demographic and behavioral Data\n",
        "jupyter: python3\n",
        "author: \"Zhengling Jiang, Colombe Tolokin, Franklin Aryee, Tien Nguyen\"\n",
        "format: \n",
        "    html:\n",
        "        toc: true\n",
        "        toc-depth: 4\n",
        "        number-sections: true\n",
        "    pdf:\n",
        "        toc: true\n",
        "        toc-depth: 4\n",
        "        number-sections: true\n",
        "editor: source\n",
        "# execute: \n",
        "#   echo: false\n",
        "bibliography: references.bib\n",
        "---"
      ],
      "id": "3912bde8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: false\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import pandera as pa\n",
        "from scipy.stats import shapiro\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.model_selection import cross_validate\n",
        "from pathlib import Path\n",
        "\n",
        "alt.renderers.enable(\"mimetype\")\n",
        "%matplotlib inline"
      ],
      "id": "57cd1762",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This project investigates whether a student's mathematics performance can be predicted using demographic and behavioral data, aiming to help educators in supporting students and tailoring educational strategies. Using a Ridge Regression model with optimized hyperparameters (alpha = 1), we achieved a strong predictive accuracy with a cross-validation score of 0.81 and evaluation metrics on the test set including an MSE of 3.83, RMSE of 1.96, and MAE of 1.27. While the model demonstrates robust performance, future work could explore non-linear models and provide confidence intervals for predictions to enhance interpretability and reliability, ultimately contributing to better educational outcomes.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Math teaches us to think logically and it also provides us with analytical and problem-solving skills. These skills can be applied to various academic and professional fields. However, student performance in mathematics can be influenced by many factors, like individual factor, social factor, and family factor. Research has shown that attributes such as study habits, age, social behaviour (alcohol consumptions, etc) and family background can significantly impact a student's academic success. Understanding these factors is crucial for improving educational outcomes. (@bitrus2016marital, @hjarnaa2023alcohol, @modi2023)\n",
        "\n",
        "In this study, we aim to address this question: **“Can we predict a student's math academic performance based on the demographic and behavioral data?”**. Answering this question is important because understanding the factors behind student performance can help teachers provide support to struggling students. Furthermore, the ability to predict academic performance could assist schools in developing educational strategies based on different backgrounds of students.\n",
        "The goal of this study is to develop a machine learning model capable of predicting student’s math performance with high accuracy.\n",
        "\n",
        "The dataset (@student_performance) used in this study contains detailed records of student demographics and behaviors, such as age, study habits, social behaviors, and family background. The target variable, mathematics performance, is measured as a continuous score reflecting students' final grade. This dataset offers a great opportunity to explore meaningful relationships between features and academic outcomes.\n",
        "\n",
        "## Methods & Results\n",
        "\n",
        "The objective here to prepare the data for our classification analysis by exploring relevant features and summarizing key insights through data wrangling and visualization.\n",
        "\n",
        "### Dataset Description\n",
        "\n",
        "The full data set contains the following columns:\n",
        "\n",
        "1. `school` - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)\n",
        "1. `sex` - student's sex (binary: 'F' - female or 'M' - male)\n",
        "1. `age` - student's age (numeric: from 15 to 22)\n",
        "1. `address` - student's home address type (binary: 'U' - urban or 'R' - rural)\n",
        "1. `famsize` - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\n",
        "1. `Pstatus` - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\n",
        "1. `Medu` - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - “ 5th to 9th grade, 3 - “ secondary education or 4 - “ higher education)\n",
        "1. `Fedu` - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - “ 5th to 9th grade, 3 - “ secondary education or 4 - “ higher education)\n",
        "1. `Mjob` - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n",
        "1. `Fjob` - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n",
        "1. `reason` - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n",
        "1. `guardian` - student's guardian (nominal: 'mother', 'father' or 'other')\n",
        "1. `traveltime` - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
        "1. `studytime` - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n",
        "1. `failures` - number of past class failures (numeric: n if 1<=n<3, else 4)\n",
        "1. `schoolsup` - extra educational support (binary: yes or no)\n",
        "1. famsup` - family educational support (binary: yes or no)\n",
        "1. `paid` - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n",
        "1. `activities` - extra-curricular activities (binary: yes or no)\n",
        "1. `nursery` - attended nursery school (binary: yes or no)\n",
        "1. `higher` - wants to take higher education (binary: yes or no)\n",
        "1. `internet` - Internet access at home (binary: yes or no)\n",
        "1. `romantic` - with a romantic relationship (binary: yes or no)\n",
        "1. `famrel` - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
        "1. `freetime` - free time after school (numeric: from 1 - very low to 5 - very high)\n",
        "1. `goout` - going out with friends (numeric: from 1 - very low to 5 - very high)\n",
        "1. `Dalc` - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
        "1. `Walc` - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
        "1. `health` - current health status (numeric: from 1 - very bad to 5 - very good)\n",
        "1. `absences` - number of school absences (numeric: from 0 to 93)\n",
        "\n",
        "These columns represent the grades:\n",
        "\n",
        "- G1 - first period grade (numeric: from 0 to 20)\n",
        "- G2 - second period grade (numeric: from 0 to 20)\n",
        "- G3 - final grade (numeric: from 0 to 20, output target)\n",
        "\n",
        "_Attribution_: The dataset variable description is copied as original from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/320/student+performance).\n",
        "\n",
        "### Data Loading, Wrangling and Summary\n",
        "\n",
        "Let's start by loading the data and have an initial view of data set structure.\n",
        "\n",
        "The file is a `.csv` file with `;` as delimiter. Let's use `pandas`to read it in.\n"
      ],
      "id": "842a2e13"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "student_performance = pd.read_csv(Path('../data/raw/student-mat.csv'))"
      ],
      "id": "957d0b49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This provides an overview of the data set with 33 columns, each representing student attributes such as age, gender, study time, grades, and parental details.\n",
        "\n",
        "Let's get some information on the data set to better understand it.\n"
      ],
      "id": "1d36d40d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "student_performance.head()"
      ],
      "id": "8bffb0b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "student_performance.info()"
      ],
      "id": "f76359a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data set contains 395 observations and 33 columns covering different aspects of student demographics, academic and behavioral traits.\n",
        "\n",
        "We can see that there is no missing values. There is not need to handle NAs.\n",
        "\n",
        "The data set includes categorical (school, sex, Mjob) and numerical (age, G1, G2, G3) features.\n",
        "\n",
        "There is a large range of features but not all of them are necessary for this analysis. Let's proceed and select only the necessary ones.\n",
        "\n",
        "Let's selected the following key columns:\n",
        "\n",
        "- Demographic attributes: sex, age\n",
        "- Academic Attributes: studytime, failures, G1, G2, G3 (grades for three terms)\n",
        "- Behavioral Attributes: goout (socializing), Dalc (weekday alcohol consumption), Walc (weekend alcohol consumption)\n",
        "\n",
        "We will also split the dataset into train and test set with a 80/20 ratio. We also set `random_state=123` for reproducibility.\n"
      ],
      "id": "f81f016f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Necessary columns\n",
        "columns = ['sex', \n",
        "           'age', \n",
        "           'studytime', \n",
        "           'failures', \n",
        "           'goout', \n",
        "           'Dalc', \n",
        "           'Walc',\n",
        "           'G1',\n",
        "           'G2',\n",
        "           'G3']\n",
        "\n",
        "subset_df = student_performance[columns]\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "    subset_df, test_size=0.2, random_state=123\n",
        ")"
      ],
      "id": "faeed605",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Validation Checks\n"
      ],
      "id": "f9c9cce3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Data validation checks:\n",
        "\n",
        "# Correct column names\n",
        "# No empty observations\n",
        "# Correct data types in each column\n",
        "# No duplicate observations\n",
        "# Correct category levels (i.e., no string mismatches or single values)\n",
        "schema = pa.DataFrameSchema(\n",
        "    {\n",
        "        \"sex\": pa.Column(str, pa.Check.isin([\"M\", \"F\"])),\n",
        "        \"age\": pa.Column(int, pa.Check.between(15, 22), nullable=False),\n",
        "        \"studytime\": pa.Column(int, pa.Check.between(1, 4), nullable=False), \n",
        "        \"failures\": pa.Column(int, pa.Check.between(0, 4), nullable=False),\n",
        "        \"goout\": pa.Column(int, pa.Check.between(1, 5), nullable=False),\n",
        "        \"Dalc\": pa.Column(int, pa.Check.between(1, 5), nullable=False),\n",
        "        \"Walc\": pa.Column(int, pa.Check.between(1, 5), nullable=False),\n",
        "        \"G1\": pa.Column(int, pa.Check.between(0, 20), nullable=False),\n",
        "        \"G2\": pa.Column(int, pa.Check.between(0, 20), nullable=False),\n",
        "        \"G3\": pa.Column(int, pa.Check.between(0, 20), nullable=False)\n",
        "    },\n",
        "    checks=[\n",
        "        pa.Check(lambda df: ~df.duplicated().any(), error=\"Duplicate rows found.\"),\n",
        "        pa.Check(lambda df: ~(df.isna().all(axis=1)).any(), error=\"Empty rows found.\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "schema.validate(subset_df, lazy=True);"
      ],
      "id": "8603322f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Missingness Not Beyond Expected Threshold\n",
        "def validate_missingness(data: pd.DataFrame, threshold: float = 0.05) -> None:\n",
        "    \"\"\"\n",
        "    Validate that missing values in the dataset do not exceed the acceptable threshold and visualize the missingness.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : pd.DataFrame\n",
        "        The dataset to check for missing values.\n",
        "    threshold : float, optional\n",
        "        The maximum allowable percentage of missing values per column (default is 0.05).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        This function does not return anything. It prints a message indicating whether missing values are within\n",
        "        the acceptable threshold and displays a heatmap of missing values in the dataset.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import pandas as pd\n",
        "    >>> df = pd.DataFrame({\n",
        "    >>>     'A': [1, 2, None, 4, 5],\n",
        "    >>>     'B': [None, None, 3, 4, 5],\n",
        "    >>>     'C': [1, 2, 3, 4, 5]\n",
        "    >>> })\n",
        "    >>> validate_missingness(df, threshold=0.1)\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"Validating missingness...\")\n",
        "    missing_percentage = data.isnull().mean()\n",
        "    above_threshold = missing_percentage[missing_percentage > threshold]\n",
        "\n",
        "    if above_threshold.empty:\n",
        "        print(\"Missingness is within the acceptable threshold.\")\n",
        "    else:\n",
        "        print(f\"Columns with missing values beyond threshold ({threshold}):\")\n",
        "        print(above_threshold)\n",
        "\n",
        "    # To visualize missingness\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(\n",
        "        data.isnull(),\n",
        "        cbar=True,\n",
        "        cmap=\"viridis\",\n",
        "        cbar_kws={'label': 'Missing Value Indicator (1 = Missing, 0 = Present)'}\n",
        "    )\n",
        "    plt.title(\"Missing Value Heatmap\")\n",
        "    plt.show()\n",
        "validate_missingness(subset_df)"
      ],
      "id": "910a0b83",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Figure 1: Missing Values Heatmap**\n"
      ],
      "id": "6e62e9e5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Target/Response Variable Follows Expected Distribution\n",
        "def validate_target_distribution(data: pd.DataFrame, target_column: str) -> None:\n",
        "    \"\"\"\n",
        "    Validate the distribution of the target variable by plotting its histogram and performing a Shapiro-Wilk test.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : pd.DataFrame\n",
        "        The dataset containing the target variable.\n",
        "    target_column : str\n",
        "        The name of the target column whose distribution is to be validated.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        This function does not return anything. It displays a histogram with a KDE of the target variable\n",
        "        and prints the result of the normality test.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import pandas as pd\n",
        "    >>> df = pd.DataFrame({\n",
        "    >>>     'target': [1.2, 2.3, 2.9, 3.5, 2.1, 3.8, 4.0, 5.1]\n",
        "    >>> })\n",
        "    >>> validate_target_distribution(df, 'target')\n",
        "    \"\"\"\n",
        "    print(\"Validating target distribution...\")\n",
        "    sns.histplot(data[target_column], \n",
        "                 kde=True, \n",
        "                 bins=20)\n",
        "    plt.title(f\"Distribution of {target_column}\")\n",
        "    plt.show()\n",
        "    stat, p = shapiro(data[target_column])\n",
        "    if p > 0.05:\n",
        "        print(f\"Target variable '{target_column}' follows a normal distribution (p={p:.4f}).\")\n",
        "    else:\n",
        "        print(f\"Target variable '{target_column}' does not follow a normal distribution (p={p:.4f}).\")\n",
        "validate_target_distribution(subset_df, target_column=\"G3\")"
      ],
      "id": "06c7c3a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Figure 2: Distribution of the target variable**\n",
        "\n",
        "#### Checking for Outliers\n",
        "\n",
        "Before proceeding with preprocessing and modeling, we check for potential outliers or anomalous values in the training dataset.\n"
      ],
      "id": "44ba1333"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def validate_no_outliers(data: pd.DataFrame, numeric_columns: list, max_cols: int = 3) -> None:\n",
        "    \"\"\"\n",
        "    Validate the presence of outliers in numeric columns using boxplots.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : pd.DataFrame\n",
        "        The dataset containing the numeric columns to be checked for outliers.\n",
        "    numeric_columns : list\n",
        "        A list of column names from `data` that contain numeric data to plot.\n",
        "    max_cols : int, optional\n",
        "        The maximum number of boxplots to display per row (default is 3).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        This function does not return anything. It displays boxplots for the specified numeric columns.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> import pandas as pd\n",
        "    >>> from matplotlib import pyplot as plt\n",
        "    >>> import seaborn as sns\n",
        "    >>> df = pd.DataFrame({\n",
        "    ...     'A': [1, 2, 3, 4, 100],\n",
        "    ...     'B': [10, 20, 30, 40, 50],\n",
        "    ...     'C': [5, 15, 25, 35, 45]\n",
        "    ... })\n",
        "    >>> validate_no_outliers(data=df, numeric_columns=['A', 'B', 'C'], max_cols=2)\n",
        "    \"\"\"\n",
        "    print(\"Validating outliers...\")\n",
        "\n",
        "    num_plots = len(numeric_columns)\n",
        "    nrows = -(-num_plots // max_cols)\n",
        "    ncols = min(num_plots, max_cols)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 10))\n",
        "\n",
        "    for ax, column in zip(axes.flatten(), numeric_columns):\n",
        "        sns.boxplot(data=subset_df, x=column, ax=ax)\n",
        "        ax.set_title(f\"Boxplot of {column}\")\n",
        "    \n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.suptitle(\"Boxplots of Numeric Columns\", fontsize=14)\n",
        "    plt.show()"
      ],
      "id": "32208158",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# checking outlier for dataset\n",
        "numeric_columns = subset_df.select_dtypes(include='number').columns\n",
        "\n",
        "validate_no_outliers(subset_df, numeric_columns)"
      ],
      "id": "90a8022b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Figure 3: Outliers visualization**\n",
        "\n",
        "There are few outliers in `failures`, `Dalc`, `age`, `studytime`, `G2`, and `G1`, as seen in the boxplots. These outliers are relatively few compared to the 395 entries, but could still influence model results. We will apply a `StandardScaler` transformation to the numeric variables, the effect of these outliers will be minimized. Therefore, we will not drop or modify these outliers at this step.\n"
      ],
      "id": "8f9c416d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import warnings\n",
        "\n",
        "def validate_anomalous_correlations(data: pd.DataFrame, target_col: str, threshold: float = 0.9, zero_tolerance: float = 1e-5):\n",
        "    \"\"\"\n",
        "    Check for anomalous correlations in a dataset:\n",
        "    - Between features and the target variable.\n",
        "    - Among features themselves.\n",
        "    - Includes checks for zero correlations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : pd.DataFrame\n",
        "        The dataset containing the target and features.\n",
        "    target_col : str\n",
        "        The name of the target column in the dataset.\n",
        "    threshold : float, optional\n",
        "        The correlation threshold above which correlations are flagged as anomalous (default is 0.9).\n",
        "    zero_tolerance : float, optional\n",
        "        The tolerance for detecting zero correlations (default is 1e-5).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary containing:\n",
        "        - 'feature_to_target': DataFrame of correlations between features and the target.\n",
        "        - 'feature_to_feature': DataFrame of correlations between features themselves.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    Warning\n",
        "        Issues warnings for high and near-zero correlations.\n",
        "    \"\"\"\n",
        "    if target_col not in data.columns:\n",
        "        raise ValueError(f\"Target column '{target_col}' not found in the dataset.\")\n",
        "    \n",
        "    features = data.drop(columns=[target_col])\n",
        "    target = data[target_col]\n",
        "\n",
        "    # Compute the full correlation matrix\n",
        "    correlation_matrix = data.corr()\n",
        "\n",
        "    # Step 1: Correlations between features and target\n",
        "    target_correlations = correlation_matrix[target_col].drop(target_col)\n",
        "\n",
        "    # Check for anomalous (high) correlations\n",
        "    anomalous_target_corrs = target_correlations[target_correlations.abs() > threshold]\n",
        "    for feature, corr in anomalous_target_corrs.items():\n",
        "        warnings.warn(\n",
        "            f\"Anomalous correlation ({corr:.2f}) between feature '{feature}' and target '{target_col}'.\"\n",
        "        )\n",
        "\n",
        "    # Check for zero correlations\n",
        "    zero_target_corrs = target_correlations[target_correlations.abs() <= zero_tolerance]\n",
        "    for feature, corr in zero_target_corrs.items():\n",
        "        warnings.warn(\n",
        "            f\"Zero or near-zero correlation ({corr:.2f}) between feature '{feature}' and target '{target_col}'.\"\n",
        "        )\n",
        "\n",
        "    # Step 2: Correlations among features\n",
        "    feature_correlations = correlation_matrix.loc[features.columns, features.columns]\n",
        "\n",
        "    # Check for anomalous (high) correlations among features\n",
        "    anomalous_feature_corrs = feature_correlations[\n",
        "        (feature_correlations.abs() > threshold) & (feature_correlations != 1)\n",
        "    ]\n",
        "    for feature1 in anomalous_feature_corrs.index:\n",
        "        for feature2 in anomalous_feature_corrs.columns:\n",
        "            if feature1 != feature2 and not pd.isna(anomalous_feature_corrs.loc[feature1, feature2]):\n",
        "                corr = anomalous_feature_corrs.loc[feature1, feature2]\n",
        "                warnings.warn(\n",
        "                    f\"Anomalous correlation ({corr:.2f}) between features '{feature1}' and '{feature2}'.\"\n",
        "                )\n",
        "                anomalous_feature_corrs.loc[feature1, feature2] = None\n",
        "                anomalous_feature_corrs.loc[feature2, feature1] = None\n",
        "\n",
        "    feature_to_target_df = pd.DataFrame({\n",
        "        'Feature': target_correlations.index,\n",
        "        'Correlation': target_correlations.values\n",
        "    }).reset_index(drop=True)\n",
        "\n",
        "    feature_to_feature_df = feature_correlations.stack().reset_index()\n",
        "    feature_to_feature_df.columns = ['Feature1', 'Feature2', 'Correlation']\n",
        "    feature_to_feature_df = feature_to_feature_df[\n",
        "        feature_to_feature_df['Feature1'] != feature_to_feature_df['Feature2']\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        'feature_to_target': feature_to_target_df,\n",
        "        'feature_to_feature': feature_to_feature_df\n",
        "    }\n",
        "\n",
        "# a warning will be issue if there are any problems\n",
        "correlation_check = validate_anomalous_correlations(\n",
        "    train_df.select_dtypes(include=\"number\"), \"G3\", threshold=0.95\n",
        ")"
      ],
      "id": "f234c674",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_df.head()"
      ],
      "id": "0f7a169a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_df.info()"
      ],
      "id": "5092f136",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's get a summary of the training set we are going to use for the analysis.\n"
      ],
      "id": "77e7a725"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train_df.describe()"
      ],
      "id": "229add4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Key takeaways from summary statistics:\n",
        "\n",
        "- Final grades `G3` range from `0` to `20`, with an average of around `10.26`.\n",
        "- The average study time is about `2.05` hours.\n",
        "- Most students have zero reported failures.\n",
        "- Alcohol consumption (Dalc and Walc) and socializing habits (goout) appear to vary across the student population.\n",
        "\n",
        "Let's create a visualization to explore the final grades `G3` distribution. We will use a histogram as it allows us to see the spread.\n",
        "\n",
        "![Distribution of Final Grades (G3)](dsci-522-group-23/results/figures/eda/g3_dist.png){#g3-dist}\n",
        "\n",
        "From @g3-dist, The histogram shows that most students achieve grades between 8 and 15, with fewer students scoring very low or very high.\n"
      ],
      "id": "1aacd583"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ally.dist(train_df).properties(title=\"Density Plot for all numeric columns\")\n",
        "fig, axes = plt.subplots(3, 3, figsize=(8, 8), sharey=False, sharex=False)\n",
        "axes = axes.flatten()\n",
        "numeric_columns = train_df.select_dtypes(include='number').columns\n",
        "for i, column in enumerate(numeric_columns):\n",
        "    dp = sns.kdeplot(data=train_df, x=column, fill=True, ax=axes[i])\n",
        "plt.tight_layout()"
      ],
      "id": "6f103d12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Figure 5: Density plot for each numeric columns (including the target `G3`)**\n",
        "\n",
        "Some interesting observations:\n",
        "\n",
        "- The distirbution of the grades `G3`, `G2`, `G1` are somewhat bell-shaped.\n",
        "- Most student do not consume alcohol, or very minimally.\n",
        "- Most student studies around 2-5 hours a week and most of them also did not fail any previous classes.\n"
      ],
      "id": "3e727304"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ally.corr(train_df).properties(title=\"Correlation matrices for each numeric column pair\")\n",
        "corr_mat = train_df.select_dtypes(include='number').corr() \\\n",
        "    .reset_index(names=\"var1\") \\\n",
        "    .melt(id_vars=\"var1\", var_name=\"var2\", value_name=\"correlation\")\n",
        "# get rid of \"duplicated\" correlation\n",
        "corr_mat = corr_mat[corr_mat['var1'] <= corr_mat['var2']].reset_index(drop=True)\n",
        "corr_mat[\"abs_corr\"] = np.abs(corr_mat[\"correlation\"])\n",
        "alt.Chart(corr_mat).mark_circle().encode(\n",
        "    alt.X(\"var1\").title(\"variable 1\"),\n",
        "    alt.Y(\"var2\").title(\"variable 2\"),\n",
        "    alt.Color(\"correlation\").scale(domain=[-1, 1], scheme=\"blueorange\"),\n",
        "    alt.Size(\"abs_corr\").legend(None)\n",
        ").properties(\n",
        "    width=250,\n",
        "    height=250,\n",
        "    title=\"Pairwise correlations between variables (including target)\"\n",
        ")"
      ],
      "id": "b53fab41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Figure 6: Correlation matrices for each numeric columns (including target `G3`)**\n",
        "\n",
        "Some interesting observations:\n",
        "\n",
        "- The grades are very correlated with one another\n",
        "- Alcohol consumptions are somewhat negatively correlated with grades\n",
        "- Study time are somewhat positively correlated with grades/\n",
        "\n",
        "### Analysis\n"
      ],
      "id": "3b52e5d8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Split features and target\n",
        "X_train, y_train = (\n",
        "    train_df.drop(columns=['G3']),\n",
        "    train_df['G3']\n",
        ")\n",
        "X_test, y_test = (\n",
        "    test_df.drop(columns=['G3']),\n",
        "    test_df['G3']\n",
        ")"
      ],
      "id": "1d8743fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train.info()"
      ],
      "id": "a4194de3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Baseline Model\n"
      ],
      "id": "0a474f26"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dr = DummyRegressor()\n",
        "\n",
        "dummy_cv = cross_validate(dr, X_train, y_train, return_train_score=True)\n",
        "pd.DataFrame(dummy_cv).agg(['mean']).T"
      ],
      "id": "ab9ee0cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define categorical and numerical columns\n"
      ],
      "id": "66ac9a5b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "categorical_feats = X_train.select_dtypes(include=['object']).columns\n",
        "numeric_feats = X_train.select_dtypes(include=['int64']).columns"
      ],
      "id": "4e087b76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply column transformers\n",
        "preprocessor = make_column_transformer(    \n",
        "    (StandardScaler(), numeric_feats),  # scaling on numeric features \n",
        "    (OneHotEncoder(drop=\"if_binary\"), categorical_feats),  # OHE on categorical features\n",
        ")\n",
        "\n",
        "#  Make pipeline\n",
        "pipe_lr = make_pipeline(preprocessor, Ridge())"
      ],
      "id": "c832232c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'ridge__alpha': [0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(pipe_lr, param_grid=param_grid, n_jobs=-1, return_train_score=True)\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "id": "9349b267",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Best score\n",
        "grid_search.best_score_"
      ],
      "id": "bdba11e7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get the best hyperparameter value\n",
        "grid_search.best_params_"
      ],
      "id": "7a11ffe4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define the best model\n",
        "best_model = grid_search.best_estimator_"
      ],
      "id": "e632a266",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.DataFrame(grid_search.cv_results_)[\n",
        "    [\n",
        "        \"mean_test_score\",\n",
        "        \"param_ridge__alpha\",\n",
        "        \"mean_fit_time\",\n",
        "        \"rank_test_score\",\n",
        "    ]\n",
        "].set_index(\"rank_test_score\").sort_index().T"
      ],
      "id": "80fcbaeb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply best model on test set\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "id": "763d604a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a dataframe to compare observed and predicted values\n",
        "comparison = pd.DataFrame({\n",
        "    \"Observed (y_test)\": y_test.values,\n",
        "    \"Predicted (y_pred)\": y_pred\n",
        "})\n",
        "comparison.head(10)"
      ],
      "id": "c314b35d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluate performance on the test set\n",
        "mse = mean_squared_error(y_test, y_pred)    # Mean squared error\n",
        "\n",
        "rmse = np.sqrt(mse) # Root Mean Squared error\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
        "\n",
        "\n",
        "# Create a data frame to display performance metrics\n",
        "metrics = pd.DataFrame({\n",
        "    \"Metric\": [\"Mean Squared Error (MSE)\", \n",
        "               \"Root Mean Squared Error (RMSE)\", \n",
        "               \"Mean Absolute Error (MAE)\"],\n",
        "    \"Value\": [mse, rmse, mae]\n",
        "})\n",
        "metrics.set_index('Metric')"
      ],
      "id": "8859a7a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Assume coefficients are stored in a matrix `coef` (predictors x responses)\n",
        "coefs = best_model.named_steps['ridge'].coef_\n",
        "feature_names = best_model.named_steps['columntransformer']\\\n",
        "    .get_feature_names_out().tolist()\n",
        "feature_names = [n.split(\"__\")[1] for n in feature_names]\n",
        "\n",
        "coefs_df = pd.DataFrame({\"features\": feature_names, \"coefs\": coefs})\n",
        "coefs_df"
      ],
      "id": "9c8adc97",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.bar(feature_names, coefs)\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Coefficient Value\")\n",
        "plt.title(\"Ridge Regression Coefficients\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "id": "72e117dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Figure 7: Ridge regression coefficients**\n",
        "\n",
        "## Results & Discussion\n",
        "\n",
        "The Ridge Regression model, with tuned hyperparameters, demonstrated well predictive capabilities on student’s math performance. The optimal hyperparameter for Ridge was found to be alpha = 1, and the best cross-validation score is approximately 0.81. This indicates a strong predictive accuracy during the model's validation phase.\n",
        "\n",
        "The Ridge coefficients suggests that student performance is most strongly influenced by prior grades, with G2 having the greatest positive impact, followed by G1. Social behaviors like going out and weekend alcohol consumption also show a smaller positive influence, while age, study time, and workday alcohol consumption have a negative effect. Failures and gender appear to have extremely minimal influence on the final grade.\n",
        "\n",
        "Based on the evaluation on the test set, the model achieved the following performance metrics:\n",
        "\n",
        "- Mean Squared Error (MSE): 3.83\n",
        "- Root Mean Squared Error (RMSE): 1.96\n",
        "- Mean Absolute Error (MAE): 1.27\n",
        "\n",
        "These metrics suggest that the model is reasonably accurate in predicting students' final grades. However, there are areas for improvement. We can explore other models which could better capture the non-linear relationships and feature interactions. Another improvement we can do is to provide confidence intervals for predictions. This approach could enhance the reliability and interpretability of predictions and help readers make more informed decisions.\n",
        "\n",
        "## References\n"
      ],
      "id": "a35d50de"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/conda/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}